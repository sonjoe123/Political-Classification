# -*- coding: utf-8 -*-
"""Highlight HTML Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iQlAVjHAgXiZUjqac7pCaHfmGikc2fsc
"""

!pip3 install emoji

from google.colab import drive
drive.mount('/content/drive')

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Preload models and tokenizers
model_paths = {
    "reddit_gpt_model": "/content/drive/MyDrive/Political Classification/reddit_gpt_model",
    "tweets_gpt_model": "/content/drive/MyDrive/Political Classification/tweets_gpt_model",
    "political_gpt_model": "/content/drive/MyDrive/Political Classification/political_gpt_model",
    "reddit_poli_model": "/content/drive/MyDrive/Political Classification/reddit_poli_model",
    "tweets_poli_model": "/content/drive/MyDrive/Political Classification/tweets_poli_model",
    "political_poli_model": "/content/drive/MyDrive/Political Classification/political_poli_model"
}

models = {}
tokenizers = {}

# Load all models and tokenizers
for name, path in model_paths.items():
    models[name] = AutoModelForSequenceClassification.from_pretrained(path, use_safetensors=True)
    tokenizers[name] = AutoTokenizer.from_pretrained(path)

import torch

def classify_sentences_batch(model, tokenizer, sentences, batch_size=8):
    """
    Classifies a batch of sentences using a given model and tokenizer on CPU.
    """
    results = []

    for i in range(0, len(sentences), batch_size):
        batch = sentences[i:i + batch_size]
        inputs = tokenizer(batch, return_tensors="pt", truncation=True, padding=True, max_length=128)
        model.eval()

        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=-1).tolist()  # Convert logits to predictions

        results.extend(predictions)

    return results  # List of predictions (1 for Liberal, 0 for Conservative)

def majority_vote_batch(sentences, batch_size=8):
    """
    Performs majority voting for a batch of sentences on CPU.
    """
    batch_results = {name: [] for name in model_paths.keys()}

    # Classify sentences with each model
    for name, model in models.items():
        tokenizer = tokenizers[name]
        predictions = classify_sentences_batch(model, tokenizer, sentences, batch_size=batch_size)
        batch_results[name] = predictions

    # Perform majority voting
    final_classifications = []
    for i in range(len(sentences)):
        votes = [batch_results[name][i] for name in model_paths.keys()]
        liberal_votes = votes.count(1)
        conservative_votes = votes.count(0)

        # Determine highlight color
        if conservative_votes >= 5:
            final_classifications.append("red")
        elif conservative_votes == 4:
            final_classifications.append("lightcoral")
        elif liberal_votes == 4:
            final_classifications.append("lightblue")
        elif liberal_votes >= 5:
            final_classifications.append("blue")
        else:
            final_classifications.append("lightgrey")  # Neutral

    return final_classifications

from bs4 import BeautifulSoup

def highlight_html(html_content, batch_size=8):
    """
    Highlights sentences in an HTML document using batch processing on CPU.
    """
    soup = BeautifulSoup(html_content, 'html.parser')

    for paragraph in soup.find_all('p'):
        sentences = [sentence.strip() for sentence in paragraph.text.split('.') if sentence.strip()]

        # Batch process sentences
        colors = []
        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i + batch_size]
            colors.extend(majority_vote_batch(batch, batch_size=batch_size))

        # Highlight sentences
        highlighted_sentences = [
            f'<span style="background-color: {color}">{sentence}.</span>'
            for sentence, color in zip(sentences, colors)
        ]

        # Update paragraph with highlighted sentences
        paragraph.clear()
        paragraph.append(BeautifulSoup(' '.join(highlighted_sentences), 'html.parser'))

    return str(soup)

import base64
from IPython.display import HTML, display
from ipywidgets import FileUpload

# Upload widget for file selection
upload_widget = FileUpload(accept='.html', multiple=False)

# Function to create a download link for the HTML content
def create_download_link(content, filename):
    b64_content = base64.b64encode(content.encode()).decode()
    href = f'<a download="{filename}" href="data:text/html;base64,{b64_content}">Click here to download the highlighted HTML file</a>'
    return href

# Function to handle uploaded file
def process_upload(change):
    try:
        # Ensure there are uploaded files to process
        if not upload_widget.value:
            print("No file uploaded.")
            return

        for filename, fileinfo in upload_widget.value.items():
            # Read the uploaded HTML file
            content = fileinfo['content'].decode('utf-8')

            # Highlight the HTML using the highlight_html function
            print(f"Processing {filename}...")  # Debug message for tracking
            highlighted_html = highlight_html(content)

            # Generate a filename for the output
            highlighted_filename = f"highlighted_{filename}"

            # Create a download link for the highlighted HTML
            download_link = create_download_link(highlighted_html, highlighted_filename)

            # Display the download link
            display(HTML(download_link))

        print("Processing complete.")  # Indicate when all files are processed
    except Exception as e:
        print(f"An error occurred: {e}")

# Attach the event handler to observe file uploads
upload_widget.observe(process_upload, names='value')

# Display the upload widget
display(upload_widget)