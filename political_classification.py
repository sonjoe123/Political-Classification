# -*- coding: utf-8 -*-
"""Political Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QsAwqM-b4daSXi5ddgvGRvFf4Wjz25Ss

# Political Classification through 6 Fine-Tuned Models
"""

!pip install transformers datasets torch

!pip3 install emoji

from google.colab import drive
drive.mount('/content/drive')

"""### Preprocessing the Datasets"""

import pandas as pd
from datasets import Dataset, load_dataset
from sklearn.model_selection import train_test_split

# Load Reddit and Tweets datasets from uploaded files
reddit_dataset = pd.read_csv('/content/drive/MyDrive/Political Classification/reddit_poliset.csv')
tweets_dataset = pd.read_csv('/content/drive/MyDrive/Political Classification/ExtractedTweets.csv')

# Preprocess Reddit dataset
reddit_dataset = reddit_dataset[['Text', 'Political Lean']].dropna()
reddit_dataset['label'] = reddit_dataset['Political Lean'].map({'Liberal': 1, 'Conservative': 0})

# Preprocess Tweets dataset
tweets_dataset = tweets_dataset[['Tweet', 'Party']]
tweets_dataset['label'] = tweets_dataset['Party'].map({'Democrat': 1, 'Republican': 0})

# Convert Reddit and Tweets datasets to Hugging Face format
reddit_hf = Dataset.from_pandas(reddit_dataset[['Text', 'label']].rename(columns={'Text': 'text'}))
tweets_hf = Dataset.from_pandas(tweets_dataset[['Tweet', 'label']].rename(columns={'Tweet': 'text'}))

# Split Reddit and Tweets datasets into train/test
def split_dataset(dataset):
    train_test = dataset.train_test_split(test_size=0.2)
    return train_test['train'], train_test['test']

reddit_train, reddit_test = split_dataset(reddit_hf)
tweets_train, tweets_test = split_dataset(tweets_hf)

# Load Political Ideologies dataset directly from Hugging Face
political_ideologies_dataset = load_dataset("JyotiNayak/political_ideologies")
political_train_test = political_ideologies_dataset['train'].train_test_split(test_size=0.2)
political_train = political_train_test['train']
political_test = political_train_test['test']

"""### Tokenizing the Datasets"""

from transformers import AutoTokenizer

# Define the model name
model_name = "kornosk/polibertweet-political-twitter-roberta-mlm"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Add a padding token if missing
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({"pad_token": "[PAD]"})
    print("Added padding token to tokenizer.")

# Tokenization function
def tokenize_data(dataset, text_column):
    """
    Tokenizes the dataset using the specified text column.
    """
    return dataset.map(
        lambda x: tokenizer(
            x[text_column],
            truncation=True,          # Truncate sequences longer than max_length
            padding="max_length",     # Pad sequences to max_length
            max_length=128            # Adjust max_length if needed
        ),
        batched=True
    )

# Tokenize Reddit dataset
reddit_train = tokenize_data(reddit_train, 'text')
reddit_test = tokenize_data(reddit_test, 'text')

# Tokenize Tweets dataset
tweets_train = tokenize_data(tweets_train, 'text')
tweets_test = tokenize_data(tweets_test, 'text')

# Tokenize Political Ideologies dataset
political_train = tokenize_data(political_train, 'statement')
political_test = tokenize_data(political_test, 'statement')

"""### Fine-Tuning Function"""

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    """
    Computes evaluation metrics for the model predictions.
    """
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

def fine_tune_and_save(model_name, train_dataset, test_dataset, save_path):
    """
    Fine-tunes a pre-trained model on a specific dataset and saves the model.
    """
    # Load pre-trained model with classification head
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2  # Binary classification
    )
    model.resize_token_embeddings(len(tokenizer))  # Align model with tokenizer

    # Define training arguments
    training_args = TrainingArguments(
        output_dir=save_path,                    # Directory to save outputs
        evaluation_strategy="epoch",            # Evaluate at the end of every epoch
        save_strategy="epoch",                  # Save model at the end of every epoch
        learning_rate=2e-5,                     # Learning rate
        per_device_train_batch_size=8,          # Batch size for training
        per_device_eval_batch_size=8,           # Batch size for evaluation
        num_train_epochs=3,                     # Number of epochs
        weight_decay=0.01,                      # Weight decay
        logging_dir=f"{save_path}/logs",        # Directory for logs
        load_best_model_at_end=True,            # Load the best model after training
        metric_for_best_model="f1",             # Use F1-score to select the best model
        greater_is_better=True                  # Higher metric is better for selection
    )

    # Initialize the Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,       # Use the PoliBERT tokenizer
        compute_metrics=compute_metrics  # Include metrics computation
    )

    # Fine-tune the model
    trainer.train()

    # Evaluate the model and display metrics
    results = trainer.evaluate()
    print("Evaluation Results:", results)

    # Save the fine-tuned model
    trainer.save_model(save_path)
    print(f"Model saved to {save_path}")

"""### Fine-Tuning poliBERT with each Dataset"""

import os
os.environ["WANDB_MODE"] = "disabled"

# Base path for saving models
base_path = "/content/drive/MyDrive/Political Classification"

model_name = "kornosk/polibertweet-political-twitter-roberta-mlm"

# Fine-tune on Reddit dataset
fine_tune_and_save(model_name, reddit_train, reddit_test, f"{base_path}/reddit_poli_model")

# Fine-tune on Tweets dataset
fine_tune_and_save(model_name, tweets_train, tweets_test, f"{base_path}/tweets_poli_model")

# Fine-tune on Political Ideologies dataset
fine_tune_and_save(model_name, political_train, political_test, f"{base_path}/political_poli_model")

from transformers import AutoModelForSequenceClassification

# Load saved models to verify
reddit_model = AutoModelForSequenceClassification.from_pretrained(f"{base_path}/reddit_poli_model")
tweets_model = AutoModelForSequenceClassification.from_pretrained(f"{base_path}/tweets_poli_model")
political_model = AutoModelForSequenceClassification.from_pretrained(f"{base_path}/political_poli_model")
print("Models loaded successfully from Google Drive!")

"""## Fine-Tuning with GPT"""

import os
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from sklearn.model_selection import train_test_split

# Dataset preparation function
def prepare_classification_dataset(dataset, text_column, label_column):
    """
    Converts a Hugging Face dataset into a format suitable for classification.
    """
    texts = [row[text_column] for row in dataset]
    labels = [row[label_column] for row in dataset]
    return texts, labels

# Prepare classification datasets
reddit_train_texts, reddit_train_labels = prepare_classification_dataset(reddit_train, text_column="text", label_column="label")
reddit_test_texts, reddit_test_labels = prepare_classification_dataset(reddit_test, text_column="text", label_column="label")

tweets_train_texts, tweets_train_labels = prepare_classification_dataset(tweets_train, text_column="text", label_column="label")
tweets_test_texts, tweets_test_labels = prepare_classification_dataset(tweets_test, text_column="text", label_column="label")

political_train_texts, political_train_labels = prepare_classification_dataset(political_train, text_column="statement", label_column="label")
political_test_texts, political_test_labels = prepare_classification_dataset(political_test, text_column="statement", label_column="label")

# Tokenizer setup
model_name = "gpt2"  # Replace with "distilgpt2" for a smaller model
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Add padding token if not already present
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({"pad_token": tokenizer.eos_token})

# Load GPT model for classification
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Resize token embeddings to account for added padding token
model.resize_token_embeddings(len(tokenizer))

# Ensure the model knows the padding token ID
model.config.pad_token_id = tokenizer.pad_token_id

# Tokenize datasets
def tokenize_classification_data(texts, labels, tokenizer, max_length=128):
    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)
    encodings["labels"] = labels
    return encodings

reddit_train_encodings = tokenize_classification_data(reddit_train_texts, reddit_train_labels, tokenizer)
reddit_test_encodings = tokenize_classification_data(reddit_test_texts, reddit_test_labels, tokenizer)

tweets_train_encodings = tokenize_classification_data(tweets_train_texts, tweets_train_labels, tokenizer)
tweets_test_encodings = tokenize_classification_data(tweets_test_texts, tweets_test_labels, tokenizer)

political_train_encodings = tokenize_classification_data(political_train_texts, political_train_labels, tokenizer)
political_test_encodings = tokenize_classification_data(political_test_texts, political_test_labels, tokenizer)

from torch.utils.data import Dataset

# Define PyTorch Dataset
class ClassificationDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

# Convert to PyTorch datasets
reddit_train_dataset = ClassificationDataset(reddit_train_encodings)
reddit_test_dataset = ClassificationDataset(reddit_test_encodings)

tweets_train_dataset = ClassificationDataset(tweets_train_encodings)
tweets_test_dataset = ClassificationDataset(tweets_test_encodings)

political_train_dataset = ClassificationDataset(political_train_encodings)
political_test_dataset = ClassificationDataset(political_test_encodings)

def fine_tune_and_save_gpt(model_name, train_dataset, test_dataset, save_path, epochs=3):
    """
    Fine-tunes a GPT model for classification and saves the model.
    """
    # Load GPT model for classification
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

    # Add padding token and resize embeddings
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": tokenizer.eos_token})
        model.resize_token_embeddings(len(tokenizer))

    # Ensure the model's padding token ID is set
    model.config.pad_token_id = tokenizer.pad_token_id

    # Define training arguments
    training_args = TrainingArguments(
        output_dir=save_path,
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=5e-5,
        per_device_train_batch_size=8,  # Adjust to 1 if memory issues persist
        per_device_eval_batch_size=8,
        num_train_epochs=epochs,
        weight_decay=0.01,
        logging_dir=f"{save_path}/logs",
        load_best_model_at_end=True,
        fp16=True,  # Enable mixed precision training
        report_to="none"  # Disable Weights & Biases logging
    )

    # Define Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    # Train the model
    trainer.train()

    # Save the fine-tuned model
    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)
    print(f"Model saved to {save_path}")

import torch

# Clear the CUDA cache
torch.cuda.empty_cache()

# Base directory for saving models
base_path = "/content/drive/MyDrive/Political Classification"

# Fine-tune on Reddit dataset
fine_tune_and_save_gpt(
    model_name=model_name,
    train_dataset=reddit_train_dataset,
    test_dataset=reddit_test_dataset,
    save_path=f"{base_path}/reddit_gpt_model"
)
torch.cuda.empty_cache()

# Fine-tune on Tweets dataset
fine_tune_and_save_gpt(
    model_name=model_name,
    train_dataset=tweets_train_dataset,
    test_dataset=tweets_test_dataset,
    save_path=f"{base_path}/tweets_gpt_model"
)
torch.cuda.empty_cache()

# Fine-tune on Political Ideologies dataset
fine_tune_and_save_gpt(
    model_name=model_name,
    train_dataset=political_train_dataset,
    test_dataset=political_test_dataset,
    save_path=f"{base_path}/political_gpt_model"
)
torch.cuda.empty_cache()